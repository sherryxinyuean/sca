{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SSA and weighted PCA on center-out (red and yellow, long delay) reaching data. Plot projections in pca and sca dimensions\n",
    "- first generate Balboa figures, then Alex.\n",
    "- Use default orthogonality and sparsity penalties for SCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy import io\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sca.models import fit_sca, weighted_pca\n",
    "from sca.util import get_sample_weights\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# add appropriate directories to search path\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Documents/Projects/Churchland/Sparsity/code/andrewPython/')\n",
    "\n",
    "from centerOutReaching_utils import calculateEpochOccupancy\n",
    "from centerOutReaching_utils import calculateOccDispersion\n",
    "from centerOutReaching_utils import calculateAI\n",
    "from centerOutReaching_utils import calculateChanceAI\n",
    "from parallelFunctions import bootstrapNeurons_SCA_PCA\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which monkey are we working with?\n",
    "monkName = 'Balboa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with rates\n",
    "load_folder='/Users/andrew/Documents/Projects/Churchland/Sparsity/data/rawRates/'\n",
    "\n",
    "# load data\n",
    "data=io.loadmat(load_folder + monkName + '_outAndBack_redYellowConds_rawRates.mat')\n",
    "\n",
    "# pull out the psths\n",
    "# data is a C x N x T tensor \n",
    "data_array=data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsample data (using a factor of 10 here)\n",
    "data_downsamp=data_array[:,:,np.arange(0,data_array.shape[2],10)]\n",
    "\n",
    "# pull out some useful numbers\n",
    "numConds,numN,trlDur = np.shape(data_downsamp)\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_concat=data_downsamp.swapaxes(0,1).reshape([data_downsamp.shape[1],data_downsamp.shape[0]*data_downsamp.shape[2]])\n",
    "\n",
    "#fr range\n",
    "fr_range=np.ptp(data_concat,axis=1)[:,None]\n",
    "\n",
    "# make a time mask\n",
    "timeMask = np.tile(np.arange(trlDur),(1,numConds)).T.flatten()\n",
    "\n",
    "# define the times we want to use for ssa/pca\n",
    "# target on: 20\n",
    "# move on:   77\n",
    "# return:    200\n",
    "trainTimes = np.arange(20,230)\n",
    "\n",
    "# define a 'training mask' for convenience \n",
    "trainMask = np.in1d(timeMask,trainTimes)\n",
    "\n",
    "#Subtract cross-condition mean\n",
    "data_scm=data_downsamp-np.mean(data_downsamp,axis=0)[None,:,:]\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_scm_concat=data_scm.swapaxes(0,1).reshape([data_scm.shape[1],data_scm.shape[0]*data_scm.shape[2]])\n",
    "\n",
    "#Soft normalize (divide each neuron by its fr range + 5)\n",
    "data_scm_norm=data_scm_concat/(fr_range+5)\n",
    "\n",
    "# mean-center the data\n",
    "dMean = np.tile(np.mean(data_scm_norm,axis=1)[:,np.newaxis],(1,data_scm_norm.shape[1]))\n",
    "data_mc = data_scm_norm - dMean\n",
    "\n",
    "# rename the data for convenience\n",
    "#Note that model requires (T x N) input rather than (N x T), which is why there are transposes below\n",
    "fit_data=np.copy(data_mc.T)\n",
    "\n",
    "# how much to weight each timestep (used by)\n",
    "sample_weights=get_sample_weights(fit_data)\n",
    "\n",
    "# number of dimensions to find\n",
    "R_est=8\n",
    "\n",
    "# Whether or not to impose hard orthogonality constraint\n",
    "hardOrthFlag = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = model.fc1.bias.detach().numpy()\n",
    "plt.plot(bias);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "start = time.time()\n",
    "model,ssa_latent, x_pred,losses=fit_sca(X=fit_data[trainMask,:],sample_weight = sample_weights[trainMask],\n",
    "                                R=R_est,orth = hardOrthFlag)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "# grab weights\n",
    "ssaW = model.fc1.weight.detach().numpy()\n",
    "\n",
    "# project all of the data into the ssa dimensions\n",
    "ssa_latent = fit_data @ ssaW.T\n",
    "\n",
    "# plot loss\n",
    "plt.plot(losses);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate reconstruction R2\n",
    "X_hat = x_pred.detach().numpy().reshape(-1,1,order = 'F')\n",
    "X = fit_data[trainMask,:].reshape(-1,1,order='F')\n",
    "\n",
    "SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "R2_ssa = 1 - (SS_res/SS_tot)\n",
    "\n",
    "# display results\n",
    "print('SSA R2: ' + str(R2_ssa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot product of the learned U weights (which are not constrained to be orthogonal)\n",
    "U_dProd = model.fc1.weight.detach().numpy()@model.fc1.weight.detach().numpy().T\n",
    "\n",
    "# plot\n",
    "fig = px.imshow(U_dProd)\n",
    "fig.update_layout(height = 300, width = 300,title = 'dot product of U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot product of the learned V weights (which may or may not be constrained to be orthogonal, depending on the 'orthFlg') \n",
    "V_dProd = model.fc2.weight.detach().numpy()@model.fc2.weight.detach().numpy().T\n",
    "\n",
    "# plot\n",
    "fig = px.imshow(abs(V_dProd),range_color = [0,0.5])\n",
    "fig.update_layout(height = 300, width = 300,title = 'dot product of V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit PCA for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_est,V_est = weighted_pca(fit_data[trainMask,:],R_est,sample_weights[trainMask])\n",
    "pca_latent = fit_data@U_est\n",
    "\n",
    "# calculate reconstruction R2\n",
    "X = fit_data[trainMask,:]\n",
    "X_hat = X @U_est @ V_est\n",
    "X_hat = X_hat.reshape(-1,1,order = 'F')\n",
    "X = X.reshape(-1,1,order = 'F')\n",
    "\n",
    "SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "R2_pca = 1 - (SS_res/SS_tot)\n",
    "\n",
    "# display results\n",
    "print('PCA R2: ' + str(R2_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some plotting colors\n",
    " - ssa is purple -> light purple\n",
    " - I used https://davidjohnstone.net/lch-lab-colour-gradient-picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ssa colors\n",
    "sca_cMap  = ['#5e0044', '#6f144e', '#812858', '#933c62', '#a5506d', '#b76477', '#c97881', '#db8c8c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot when reordering by time of maximal influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate across-condition variance of each projection as a function of time\n",
    "\n",
    "# reshape both latents to be size T x C x K \n",
    "rs_sca_latent = np.reshape(ssa_latent,(-1,8,R_est),order = 'F')\n",
    "\n",
    "# calculate across condition variance\n",
    "sca_var = np.var(rs_sca_latent,axis = 1)\n",
    "\n",
    "# find peak occupancy of each dimension\n",
    "pkIdx = np.argmax(sca_var,axis = 0)\n",
    "\n",
    "# define plotting order\n",
    "sca_order = np.argsort(pkIdx)\n",
    "\n",
    "# resort ssa_latents by time of maximum occupancy\n",
    "rs_sca_latent = rs_sca_latent[:,:,sca_order]\n",
    "\n",
    "# do the same for the pca projections\n",
    "rs_pca_latent = np.reshape(pca_latent,(-1,8,R_est),order = 'F')\n",
    "\n",
    "pca_var = np.var(rs_pca_latent,axis = 1)\n",
    "pkIdx = np.argmax(pca_var,axis = 0)\n",
    "pca_order = np.argsort(pkIdx)\n",
    "rs_pca_latent = rs_pca_latent[:,:,pca_order]\n",
    "\n",
    "# define some useful time points\n",
    "tgt_idx=20\n",
    "move_idx=77\n",
    "ret_idx=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternatively, order latents by the time that occupancy crosses some threshold\n",
    "#\n",
    "# # reshape both latents to be size T x C x K\n",
    "# rs_sca_latent = np.reshape(ssa_latent,(-1,8,R_est),order = 'F')\n",
    "#\n",
    "# # calculate across condition variance\n",
    "# sca_var = np.var(rs_sca_latent,axis = 1)\n",
    "#\n",
    "# # for each occupancy trace, calculate when it crosses some (low) threshold\n",
    "# threshTime_sca = np.zeros(R_est)\n",
    "# threshold = 0.01\n",
    "# for ii in range(R_est):\n",
    "#     threshTime_sca[ii] = int(np.argwhere(sca_var[:,ii] > threshold)[0])\n",
    "#\n",
    "# # sort this list\n",
    "# sca_order = np.argsort(threshTime_sca)\n",
    "#\n",
    "# # re-arrange latents\n",
    "# rs_sca_latent = rs_sca_latent[:,:,sca_order]\n",
    "#\n",
    "# # now do the same for pca\n",
    "# rs_pca_latent = np.reshape(pca_latent,(-1,8,R_est),order = 'F')\n",
    "#\n",
    "# pca_var = np.var(rs_pca_latent,axis = 1)\n",
    "#\n",
    "# threshTime_pca = np.zeros(R_est)\n",
    "# threshold = 0.01\n",
    "# for ii in range(R_est):\n",
    "#     threshTime_pca[ii] = int(np.argwhere(pca_var[:,ii] > threshold)[0])\n",
    "#\n",
    "# # sort this list\n",
    "# pca_order = np.argsort(threshTime_pca)\n",
    "# rs_pca_latent = rs_pca_latent[:,:,pca_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory for our new figures\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/defaultSCAParameters/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# range for y axis\n",
    "yRange = [-1.8,1.8]\n",
    "\n",
    "fig = make_subplots(rows=R_est,cols = 1,shared_xaxes = True,vertical_spacing = 0)\n",
    "\n",
    "for ii in range(R_est):\n",
    "\n",
    "    for jj in range(numConds):\n",
    "        latTrace = go.Scatter(y = rs_sca_latent[:,jj,ii], line = go.scatter.Line(color = sca_cMap[jj],width = 2.5),showlegend = False)\n",
    "        fig.add_trace(latTrace,row = ii+1,col=1)\n",
    "        if ii == (R_est-1):\n",
    "            fig.add_vline(x = tgt_idx,row = ii+1,col = 1, line_color = 'black')\n",
    "            fig.add_vline(x = move_idx,row = ii+1,col = 1, line_color = 'black')\n",
    "            fig.add_vline(x = ret_idx,row = ii+1,col = 1, line_color = 'black')\n",
    "\n",
    "            #\n",
    "    # add a vertical line for scale\n",
    "    scaleLine = go.Scatter(x = [0,0],y = [-1,1],showlegend = False,mode = 'lines',\n",
    "                            line = go.scatter.Line(color = 'black',width = 5))\n",
    "    fig.add_trace(scaleLine,row = ii+1,col = 1)\n",
    "\n",
    "\n",
    "fig.update_layout(height = 2000,width =600,title = 'SCA ' + monkName,title_font_color = 'black',\n",
    "                  paper_bgcolor = 'white',\n",
    "                  plot_bgcolor = 'white')\n",
    "fig.update_yaxes(showgrid = False,zeroline = False,visible = False,range = yRange)\n",
    "fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,visible = False)\n",
    "fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,\n",
    "                 ticks = 'outside',tickvals = [0,50],ticktext = ['0','500'],visible = True,row = R_est,col = 1)\n",
    "\n",
    "\n",
    "# save\n",
    "#fig.write_image(figDir + monkName + 'SCA_' + str(R_est) + 'dims.pdf')\n",
    "#fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the fractional occupancy for each epoch for each dimension\n",
    "    - for reference:\n",
    "    tgt_idx=20\n",
    "    move_idx=77\n",
    "    ret_idx=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vector of times, where 0 is the start of the trial (not target onset)\n",
    "time = list(range(rs_sca_latent.shape[0]))\n",
    "\n",
    "# define a prep. time\n",
    "# outward reach\n",
    "prepTime = list(range(tgt_idx,move_idx))\n",
    "# return reach\n",
    "prepTime.extend(list(range(ret_idx - 30, ret_idx)))\n",
    "\n",
    "# execution time\n",
    "# outward reach\n",
    "moveTime = list(range(move_idx,move_idx + 30))\n",
    "# return reach\n",
    "moveTime.extend(list(range(ret_idx, ret_idx + 30 )))\n",
    "\n",
    "# posture time\n",
    "postTime = list(range(move_idx + 30,ret_idx - 30))\n",
    "\n",
    "# plot all of our chosen times to make sure they make sense\n",
    "fig = go.Figure(go.Scatter(y = np.isin(time,prepTime), name = 'prep'))\n",
    "fig.add_trace(go.Scatter(y = np.isin(time,moveTime), name = 'move'))\n",
    "fig.add_trace(go.Scatter(y = np.isin(time,postTime),name = 'post'))\n",
    "\n",
    "fig.add_vline(x = tgt_idx, line_color = 'white',annotation_text = 'target on',annotation_position = 'top')\n",
    "fig.add_vline(x = move_idx, line_color = 'white',annotation_text = 'move out',annotation_position = 'top')\n",
    "fig.add_vline(x = ret_idx, line_color = 'white',annotation_text = 'return move',annotation_position = 'top')\n",
    "\n",
    "fig.update_layout(height = 400,width = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fractional occupancy\n",
    "fractOcc_sca = calculateEpochOccupancy(rs_sca_latent,prepTimes = prepTime, moveTimes = moveTime, postTimes = postTime, projTimes = time)\n",
    "fractOcc_pca = calculateEpochOccupancy(rs_pca_latent,prepTimes = prepTime, moveTimes = moveTime, postTimes = postTime, projTimes = time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate dispersion for each dimension (sum of absolute difference between each fractional occupancy)\n",
    "disp_sca = calculateOccDispersion(fractOcc_sca)\n",
    "disp_pca = calculateOccDispersion(fractOcc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "zMin = 0\n",
    "zMax = 0.7\n",
    "\n",
    "# initialize a figure\n",
    "fig = make_subplots(rows=1,cols = 2,shared_xaxes = False,shared_yaxes = False,horizontal_spacing = 0.1, subplot_titles = ['SCA','PCA'])\n",
    "\n",
    "# sca first\n",
    "htPlt = go.Heatmap(go.Heatmap(z = fractOcc_sca,zmin = zMin, zmax = zMax, colorscale = 'Brwnyl'))\n",
    "fig.add_trace(htPlt, row=1, col=1)\n",
    "\n",
    "# pca\n",
    "htPlt = go.Heatmap(go.Heatmap(z = fractOcc_pca,zmin = zMin, zmax = zMax, colorscale = 'Brwnyl'))\n",
    "fig.add_trace(htPlt, row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(autorange = 'reversed',title = 'dimension')\n",
    "fig.update_xaxes(title = 'epoch',tickmode = 'array',\n",
    "                 tickvals = [0,1,2],\n",
    "                 ticktext = ['prep.','exec.','post.'])\n",
    "\n",
    "fig.update_layout(width = 800,height = 500,title = 'Epoch Sparsity ' + monkName)\n",
    "\n",
    "# save figure\n",
    "#fig.write_image(figDir + monkName + '_SCAPCA_' + str(R_est) + 'dims_epochSparsityMap.pdf')\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the max value of each row\n",
    "print(np.max(fractOcc_sca,axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate PCA and SCA latents from bootstrapped neuron populations\n",
    "    - using the same population for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies to use as arguments for function we're going to parallelize\n",
    "# copy R_est, fit_data, and trainMask\n",
    "\n",
    "# number of bootstraps\n",
    "numBoots = 500\n",
    "\n",
    "inputList = []\n",
    "for ii in range(numBoots):\n",
    "    inputList.append([R_est,fit_data,trainMask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bootstrapping function\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "# set up parallel pool to use all available workers\n",
    "pool = Pool()\n",
    "\n",
    "# bootstrap\n",
    "output = pool.starmap(bootstrapNeurons_SCA_PCA,inputList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the pool\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse outputs and save\n",
    "numReps = len(output)\n",
    "sca_latents_all = np.zeros((fit_data.shape[0],R_est,numReps))\n",
    "pca_latents_all = np.zeros((fit_data.shape[0],R_est,numReps))\n",
    "\n",
    "\n",
    "for ii in range(numReps):\n",
    "    sca_latents_all[:,:,ii] = output[ii][0]\n",
    "    pca_latents_all[:,:,ii] = output[ii][1]\n",
    "\n",
    "\n",
    "# save everything\n",
    "saveDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/data/reaching/sca_pca_defaultParams_bootstrap/'\n",
    "np.save(saveDir + monkName + '_sca_pca_bootstrappedLatents_' + str(R_est) + 'Dims_v2.npy', {'sca_latents': sca_latents_all,'pca_latents':pca_latents_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the latents if we need to\n",
    "saveDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/data/reaching/sca_pca_defaultParams_bootstrap/'\n",
    "data = np.load(saveDir + monkName + '_sca_pca_bootstrappedLatents_8Dims.npy',allow_pickle=True)\n",
    "data = data.item()\n",
    "sca_latents = data['sca_latents']\n",
    "pca_latents = data['pca_latents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each set of sca (and pca) latents, calculate epoch sparsity matrix, then median sparsity value (need to come up with a better name) across dimensions\n",
    "\n",
    "# initialize two vectors to hold results\n",
    "numBoots = sca_latents.shape[2]\n",
    "sparseIdx_sca = np.zeros(numBoots)\n",
    "sparseIdx_pca = np.zeros(numBoots)\n",
    "\n",
    "# cycle through bootstraps\n",
    "for ii in range(numBoots):\n",
    "\n",
    "    # reshape pca and sca projections to be T x C x K\n",
    "    sca_rs = sca_latents[:,:,ii].reshape(-1,8,R_est,order = 'F')\n",
    "    pca_rs = pca_latents[:,:,ii].reshape(-1,8,R_est,order = 'F')\n",
    "\n",
    "\n",
    "    # calculate fractional occupancy\n",
    "    fractOcc_sca = calculateEpochOccupancy(sca_rs,prepTimes = prepTime, moveTimes = moveTime, postTimes = postTime, projTimes = time)\n",
    "    fractOcc_pca = calculateEpochOccupancy(pca_rs,prepTimes = prepTime, moveTimes = moveTime, postTimes = postTime, projTimes = time)\n",
    "\n",
    "    # calculate dispersion for each dimension (sum of absolute difference between each fractional occupancy row)\n",
    "    disp_sca = calculateOccDispersion(fractOcc_sca)\n",
    "    disp_pca = calculateOccDispersion(fractOcc_pca)\n",
    "\n",
    "    # save mean across dimensions\n",
    "    sparseIdx_sca[ii] = np.copy(np.mean(disp_sca))\n",
    "    sparseIdx_pca[ii] = np.copy(np.mean(disp_pca))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sca and pca colors\n",
    "eBarColors = ['#5e0044','#a79ba4']\n",
    "\n",
    "# combine data into lists to make life easy\n",
    "sparseIdx = [np.copy(sparseIdx_sca), np.copy(sparseIdx_pca)]\n",
    "\n",
    "# define some plotting parameters\n",
    "eBarThickness = 5\n",
    "eBarWidth = 10\n",
    "markerSize = 15\n",
    "markerLineWidth = 2\n",
    "\n",
    "# sca\n",
    "fig = go.Figure(data = go.Scatter(\n",
    "    x = [0],y = [np.mean(sparseIdx[0])],\n",
    "    error_y = dict(\n",
    "        type = 'data',\n",
    "        array = [np.std(sparseIdx[0])],visible = True,thickness = eBarThickness,width = eBarWidth),\n",
    "    marker = dict(\n",
    "        color = eBarColors[0],\n",
    "        size = markerSize,\n",
    "        line = dict(\n",
    "            color = 'black',\n",
    "            width = markerLineWidth\n",
    "        )\n",
    "    ),showlegend = False\n",
    "    )\n",
    ")\n",
    "\n",
    "# pca\n",
    "pcaTrace = go.Scatter(\n",
    "    x = [1],y = [np.mean(sparseIdx[1])],\n",
    "    error_y = dict(\n",
    "        type = 'data',\n",
    "        array = [np.std(sparseIdx[1])],visible = True,thickness = eBarThickness,width = eBarWidth),\n",
    "    marker = dict(\n",
    "        color = eBarColors[1],\n",
    "        size = markerSize,\n",
    "        line = dict(\n",
    "            color = 'black',\n",
    "            width = markerLineWidth\n",
    "        )\n",
    "    ),showlegend = False\n",
    ")\n",
    "fig.add_trace(pcaTrace)\n",
    "\n",
    "# clean up figure\n",
    "fig.update_layout(height =500,width =350,title = 'SCA vs. PCA epoch sparsity ' + monkName,title_font_color = 'black',\n",
    "                  paper_bgcolor = 'white',\n",
    "                  plot_bgcolor = 'white')\n",
    "fig.update_yaxes(showgrid = False,zeroline = False,visible = False)\n",
    "fig.update_yaxes(color = 'black',ticks = 'outside',visible = True,showline = True,linewidth = 1.5,tickwidth = 1.5)\n",
    "fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,visible = False)\n",
    "fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,\n",
    "                 ticks = 'outside',tickvals = [0,1],ticktext = ['SCA','PCA'],visible = True,tickwidth = 1.5)\n",
    "\n",
    "# save figure\n",
    "# fig.write_image(figDir + monkName + '_SCA_vs_PCA_' + str(R_est) + 'dims_bootstrap_epochSparsitySummary.pdf')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pVal for sparseIdx_sca < sparseIdx_pca\n",
    "scaBigger = [1 if x < y else 0 for x,y in zip(sparseIdx[0],sparseIdx[1])]\n",
    "pVal = sum(scaBigger) / len(scaBigger)\n",
    "print('bootstrap pVal: ' + str(pVal) + ' (' + str(numBoots) +' bootstraps)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the (pairwise) alignment indices between prep_out, move_out, posture, prep_return, and move_return\n",
    "    for reference:\n",
    "    tgt_idx=20\n",
    "    move_idx=77\n",
    "    ret_idx=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the time periods for each epoch\n",
    "\n",
    "# prep\n",
    "prepOut = list(np.arange(tgt_idx,move_idx - 10))\n",
    "prepRtn = list(np.arange(ret_idx - 30,ret_idx - 10))\n",
    "\n",
    "\n",
    "# move\n",
    "moveOut = list(np.arange(move_idx,move_idx + 20))\n",
    "moveRtn = list(np.arange(ret_idx,ret_idx + 20))\n",
    "\n",
    "# posture\n",
    "post = list(np.arange(move_idx + 30,ret_idx - 50))\n",
    "\n",
    "# place everything in a list\n",
    "epochTimes = [prepOut, moveOut, post, prepRtn, moveRtn]\n",
    "numEpochs = len(epochTimes)\n",
    "\n",
    "# number of dimensions to use for alignemnt index calculation\n",
    "numDims = 10\n",
    "\n",
    "# make a 'numEpochs' x 'numEpochs' matrix to hold results\n",
    "allAI = np.zeros((numEpochs,numEpochs))\n",
    "\n",
    "# cycle through all epochs\n",
    "for ii in range(numEpochs):\n",
    "    for jj in range(numEpochs):\n",
    "\n",
    "        # grab data from epoch ii\n",
    "        X1 = fit_data[np.isin(timeMask,epochTimes[ii]),:]\n",
    "\n",
    "        # data from epoch jj\n",
    "        X2 = fit_data[np.isin(timeMask,epochTimes[jj]),:]\n",
    "\n",
    "        # calculate alignment index\n",
    "        allAI[ii,jj] = calculateAI(X1,X2,numDims)\n",
    "\n",
    "# calculate change AI\n",
    "chanceAI = calculateChanceAI(fit_data,numDims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate median chance AI\n",
    "medChanceAI = np.median(chanceAI)\n",
    "print('chance AI: ' + str(medChanceAI))\n",
    "\n",
    "# plot results\n",
    "fig = go.Figure(go.Heatmap(z = allAI,zmin = 0, zmax = 1,colorscale = 'Electric'))\n",
    "\n",
    "# add labels\n",
    "fig.update_layout(yaxis = dict(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [0,1,2,3,4],\n",
    "    ticktext = ['prep out','move out','posture','prep rtn','move rtn']),\n",
    "                xaxis = dict(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [0,1,2,3,4],\n",
    "    ticktext = ['prep out','move out','posture','prep rtn','move rtn'],\n",
    "    tickcolor = 'black'),\n",
    "                width = 500,height = 500,title = monkName + ' alignment indices')\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# save figure (as html).\n",
    "fig.write_html(figDir + monkName + '_' + str(R_est) + 'dims_aligmentIndex.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a figure\n",
    "fig = make_subplots(rows=1,cols = numDimCount,shared_xaxes = True,shared_yaxes = True,horizontal_spacing = 0.01, subplot_titles = [str(x) + ' dims' for x in numDimsInRun] )\n",
    "\n",
    "# cycle through runs with different number of dimensions\n",
    "for dd in range(numDimCount):\n",
    "\n",
    "    # reshape\n",
    "    tempDisp = disp_all_rs[:,dd]\n",
    "    tempDisp = tempDisp.reshape(numSparse,numOrthog,order = 'F')\n",
    "\n",
    "    # plot\n",
    "    tempImage = go.Heatmap(z = tempDisp, zmin = 0,zmax = 2,x = orthogLambdas,y = sparsityLambdas,colorscale = 'Magma')\n",
    "    fig.add_trace(tempImage, row = 1, col=dd+1)\n",
    "\n",
    "\n",
    "\n",
    "# fix some odd bug with the y axis tick labels\n",
    "fig.update_layout(yaxis = dict(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [1e-4, 1e-2, 1, 100]))\n",
    "\n",
    "fig.update_xaxes(type = 'log')\n",
    "fig.update_yaxes(type = 'log')\n",
    "fig.update_layout(height = 600,width = 1000,xaxis_title = 'orthog lambda',yaxis_title = 'sparse lambdas',coloraxis_colorbar = dict(title = 'mean dispersion'),title = 'Epoch Dispersion')\n",
    "\n",
    "fig.write_image(figDir + monkName + 'orth_sparse_lambdaSweeps_epochDispersion.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regress sca latents against gamal latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with rates\n",
    "load_folder='/Users/andrew/Documents/Projects/Churchland/Sparsity/data/reaching/'\n",
    "\n",
    "# load data\n",
    "data=io.loadmat(load_folder + monkName + '_gamalLoadings.mat')\n",
    "\n",
    "# pull out prep, move, and posture projections\n",
    "# (N x k)\n",
    "gPrepProj=data['prepProj'][:,:,None]\n",
    "gMoveProj=data['moveProj'][:,:,None]\n",
    "gPostProj=data['postProj'][:,:,None]\n",
    "\n",
    "# concatenate all projections\n",
    "gProj = np.concatenate((gPrepProj,gMoveProj,gPostProj),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "numBoots = pca_latents.shape[2]\n",
    "\n",
    "# number of SCA dimensions\n",
    "R_est = sca_latents.shape[1]\n",
    "\n",
    "# initialize vector to hold R2\n",
    "# each column corresponds to the reconstruction from the prep, move, or posture projections\n",
    "allPCA_R2 = np.zeros((numBoots,R_est,3))\n",
    "\n",
    "# cycle through bootstrap repetitions\n",
    "for ii in np.arange(numBoots):\n",
    "\n",
    "    # cycle through each pca dimension\n",
    "    for jj in range(pca_latents.shape[1]):\n",
    "\n",
    "        # pull out bootstrap latents\n",
    "        Y = pca_latents[:,jj,ii]\n",
    "\n",
    "        # get B (regression weights) from prep, move, and posture projections\n",
    "        for e in np.arange(3):\n",
    "\n",
    "            # make life a bit easier and just pull out the gamal projections we want\n",
    "            X = gProj[:,:,e]\n",
    "\n",
    "            # regress\n",
    "            B = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "            # reconstruct Y\n",
    "            Y_hat = X @ B\n",
    "\n",
    "            # calculate R2\n",
    "            ss_tot = np.sum((Y - np.mean(Y,axis = 0))**2, axis = 0)\n",
    "            ss_res = np.sum((Y - Y_hat)**2,axis = 0)\n",
    "            allPCA_R2[ii,jj,e] = 1 - (ss_res/ss_tot)\n",
    "\n",
    "# take max R2 across each epoch\n",
    "maxR2_pca = np.max(allPCA_R2,axis = 2)\n",
    "\n",
    "# redo regression analysis for ssa projections\n",
    "# initialize vector to hold R2\n",
    "allSCA_R2 = np.zeros((numBoots,R_est,3))\n",
    "\n",
    "# cycle through bootstrap repetitions\n",
    "for ii in np.arange(numBoots):\n",
    "\n",
    "    # cycle through each sca dimension\n",
    "    for jj in range(sca_latents.shape[1]):\n",
    "\n",
    "        # pull out bootstrap latents\n",
    "        Y = sca_latents[:,jj,ii]\n",
    "\n",
    "        # get B (regression weights) from prep, move, and posture projections\n",
    "        for e in np.arange(3):\n",
    "\n",
    "            # make life a bit easier and just pull out the gamal projections we want\n",
    "            X = gProj[:,:,e]\n",
    "\n",
    "            # regress\n",
    "            B = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "            # reconstruct Y\n",
    "            Y_hat = X @ B\n",
    "\n",
    "            # calculate R2\n",
    "            ss_tot = np.sum((Y - np.mean(Y,axis = 0))**2, axis = 0)\n",
    "            ss_res = np.sum((Y - Y_hat)**2,axis = 0)\n",
    "            allSCA_R2[ii,jj,e] = 1 - (ss_res/ss_tot)\n",
    "\n",
    "# take max R2 across each epoch\n",
    "maxR2_sca = np.max(allSCA_R2,axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x1 > x2\n",
    "print(np.sum(y)/x1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = maxR2_pca.reshape(-1,1,order='F')\n",
    "x2 = maxR2_sca.reshape(-1,1,order='F')\n",
    "m1 = np.mean(x1)\n",
    "s1 = np.std(x1)\n",
    "m2 = np.mean(x2)\n",
    "s2 = np.std(x2)\n",
    "plt.errorbar(0,m1,s1,color='b');\n",
    "plt.errorbar(1,m2,s2,color='r');\n",
    "plt.ylim([0,1]);\n",
    "plt.xlim([-0.5,1.5]);\n",
    "\n",
    "# # save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_ssaVsPCA_reconError_test.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change color of axis labels so we can see them in the pdf\n",
    "plt.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "\n",
    "# reshape pca reconstructions\n",
    "pcaR2_recon = np.reshape(maxR2_pca,[-1,1],order = 'F')\n",
    "\n",
    "\n",
    "# plot pca results with a small amount of jitter around 1\n",
    "pcaLoc = np.random.normal(loc = 1,scale = 0.03,size = (numBoots*R_est,1))\n",
    "\n",
    "# plot means\n",
    "plt.plot(pcaLoc,pcaR2_recon,'o',color = (0.4,0.4,0.4),ms = 4,alpha = 0.01);\n",
    "\n",
    "# plot the mean and std of pca performance\n",
    "plt.errorbar(1,np.mean(pcaR2_recon),np.std(pcaR2_recon),color = 'k',lw = 3,zorder = 3)\n",
    "plt.plot(1,np.mean(pcaR2_recon),'o',color = 'k',ms = 8,zorder = 3);\n",
    "\n",
    "\n",
    "# reshape pca reconstructions\n",
    "scaR2_recon = np.reshape(maxR2_sca,[-1,1],order = 'F')\n",
    "\n",
    "# get jittered position for ssa reconstructions\n",
    "ssaLoc = np.random.normal(loc = 2,scale = 0.01,size = (numBoots*R_est,1))\n",
    "plt.plot(ssaLoc,scaR2_recon,'o',color = 'purple',alpha = 0.01);\n",
    "\n",
    "# plot the mean and std of ssa performance\n",
    "plt.errorbar(2,np.mean(scaR2_recon),np.std(scaR2_recon) ,color = 'black',lw = 3)\n",
    "plt.plot(2,np.mean(scaR2_recon),'o',color = 'black',ms = 8);\n",
    "\n",
    "\n",
    "\n",
    "# clean up\n",
    "plt.xlim((0.5, 2.5));plt.ylim((0,1));\n",
    "plt.xticks(np.array([1,2]),('PCA','SSA'));\n",
    "plt.yticks(np.array([0,0.5, 1]));\n",
    "plt.title(monkName);\n",
    "plt.ylabel('R2');\n",
    "\n",
    "\n",
    "# # save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "#plt.savefig(figDir + monkName + '_ssaVsPCA_reconError.pdf',dpi = 'figure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pcaR2_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle through number of dimensions and calculate R2 of reconstructed neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions to test\n",
    "numDims = np.arange(5,55,5)\n",
    "\n",
    "# initialize vectors to hold results\n",
    "R2_sca = np.zeros(numDims.shape[0]) + np.nan\n",
    "R2_pca = np.zeros(numDims.shape[0]) + np.nan\n",
    "\n",
    "# cycle through number of dimensions\n",
    "for ii,jj in enumerate(numDims):\n",
    "\n",
    "    # fit SCA\n",
    "    model,ssa_latent, x_pred,losses=fit_sca(X=fit_data[trainMask,:],sample_weight = sample_weights[trainMask],\n",
    "                                    R=jj,orth = hardOrthFlag)\n",
    "    # calculate reconstruction R2\n",
    "    X_hat = x_pred.detach().numpy().reshape(-1,1,order = 'F')\n",
    "    X = fit_data[trainMask,:].reshape(-1,1,order='F')\n",
    "\n",
    "    SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "    SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "    R2_sca[ii] = 1 - (SS_res/SS_tot)\n",
    "\n",
    "    # fit PCA\n",
    "    U_est,V_est = weighted_pca(fit_data[trainMask,:],jj,sample_weights[trainMask])\n",
    "    pca_latent = fit_data@U_est\n",
    "\n",
    "    # calculate reconstruction R2\n",
    "    X = fit_data[trainMask,:]\n",
    "    X_hat = X @U_est @ V_est\n",
    "    X_hat = X_hat.reshape(-1,1,order = 'F')\n",
    "    X = X.reshape(-1,1,order = 'F')\n",
    "\n",
    "    SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "    SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "    R2_pca[ii] = 1 - (SS_res/SS_tot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "# change color of axis labels so we can see them in the pdf\n",
    "plt.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "\n",
    "# plot sca results\n",
    "plt.plot(numDims, R2_sca,'k',label='sca')\n",
    "\n",
    "# and pca results\n",
    "plt.plot(numDims,R2_pca,color=[0.5,0.5,0.5],label='pca')\n",
    "\n",
    "# clean up\n",
    "plt.xlim((0, 50));plt.ylim((0,1));\n",
    "plt.xticks(np.array([0,25,50]));\n",
    "plt.yticks(np.array([0,0.5, 1]));\n",
    "plt.title(monkName);\n",
    "plt.ylabel('R2');\n",
    "\n",
    "\n",
    "# # save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_scaVsPCA_reconError.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
