{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook demonstrates the use of Sparse Component Analysis (SCA) using a center-out reaching dataset\n",
    "\n",
    "Data are trial-averaged firing rates of individual neurons collected from motor cortex (primary motor and dorsal premotor) of a non-human primate.\n",
    "\n",
    "For this task, the monkey began each trial by touching a central touch-point. A peripheral target was shown, and after a variable, unpredictable delay period, a go cue was delivered. After capturing the peripheral target, the monkey recieved a juice reward, and returned his hand to the touch-point to begin the next trial (see [Lara et al., 2018](https://pubmed.ncbi.nlm.nih.gov/30132759/))\n",
    "\n",
    "Data have been aligned to target onset, outward movement onset, and return reach onset.\n",
    "\n",
    "'data' is a Condition x Neuron x Time tensor of trial-averaged firing rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import various packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sca.models import SCA, WeightedPCA\n",
    "from sca.util import get_sample_weights, get_accuracy\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from nimfa import Nmf, Snmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local path to the directory\n",
    "    - fill in the path to whererever you've saved 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = ## FILL IN PATH ##\n",
    "\n",
    "# local_path = '../datasets/'\n",
    "# local_path = '../data/'\n",
    "\n",
    "load_folder = '../data/'\n",
    "\n",
    "monkName = 'Balboa'\n",
    "\n",
    "data=io.loadmat(load_folder + monkName + '_outAndBack_redYellowConds_rawRates.mat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=io.loadmat(local_path + 'monkeyB_reaching.mat')\n",
    "\n",
    "# pull out the PSTHs\n",
    "# data is a C x N x T tensor \n",
    "data_array=data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "We're going to perform two standard (to this dataset) pre-processing steps:\n",
    "\n",
    "   1. soft-normalize the rates (firing rate range across all conditions and times + 5)\n",
    "        This step has the effect of preventing the recovered factors from being dominated by a few high firing-rate neurons while also minimizing the impact of very low firing rate neurons\n",
    "\n",
    "   2. subtract off the cross-condition mean\n",
    "        The largest signal (in terms of variance) in motor cortex during reaching is a condition-invariant 'trigger signal' (see [Kaufman et al., 2016](https://pubmed.ncbi.nlm.nih.gov/27761519/) for further discussion).\n",
    "        Because we are often more interested in the condition-specific signals, we typically subtract off this trigger signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsample data to speed up SCA (using a factor of 10 here)\n",
    "data_downsamp=data_array[:,:,np.arange(0,data_array.shape[2],10)]\n",
    "\n",
    "# pull out some useful numbers\n",
    "numConds,numN,trlDur = np.shape(data_downsamp)\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_concat=data_downsamp.swapaxes(0,1).reshape([data_downsamp.shape[1],data_downsamp.shape[0]*data_downsamp.shape[2]])\n",
    "\n",
    "#fr range\n",
    "fr_range=np.ptp(data_concat,axis=1)[:,None]\n",
    "\n",
    "# make a time mask\n",
    "timeMask = np.tile(np.arange(trlDur),(1,numConds)).T.flatten()\n",
    "\n",
    "# define the times we want to use for sca/pca\n",
    "# target on: 20\n",
    "# move on:   77\n",
    "# return:    200\n",
    "trainTimes = np.arange(20,230)\n",
    "\n",
    "# define a 'training mask' for convenience \n",
    "trainMask = np.in1d(timeMask,trainTimes)\n",
    "\n",
    "#Subtract cross-condition mean\n",
    "# data_scm=data_downsamp-np.mean(data_downsamp,axis=0)[None,:,:]\n",
    "data_scm=data_downsamp\n",
    "\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_concat=data_scm.swapaxes(0,1).reshape([data_scm.shape[1],data_scm.shape[0]*data_scm.shape[2]])\n",
    "\n",
    "#Soft normalize (divide each neuron by its fr range + 5)\n",
    "data_norm=data_concat/(fr_range+5)\n",
    "\n",
    "data_snm_norm=data_norm-np.mean(data_norm,axis=1)[:,None]\n",
    "\n",
    "\n",
    "# rename the data for convenience\n",
    "#Note that model requires (T x N) input rather than (N x T), which is why there are transposes below\n",
    "fit_data=np.copy(data_snm_norm.T)\n",
    "fit_data_pos=np.copy(data_norm.T)\n",
    "\n",
    "\n",
    "# how much to weight each timestep (used by)\n",
    "sample_weights=get_sample_weights(fit_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some SCA parameters\n",
    "SCA has three hyperparameters:\n",
    "\n",
    "   number of requested factors (R_est)\n",
    "   lam_orthog: determines the degree to which non-orthogonal dimensions are penalized\n",
    "   lam_sparse: determines the degree to which non-sparse factors are punished.\n",
    "\n",
    "For this analysis, we are going to use the default values for lam_orthog and lam_sparse.\n",
    "\n",
    "Across all examined datasets, the recovered SCA factors vary litte across different hyperparameter choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions to find\n",
    "R_est=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SCA\n",
    "sca=SCA(n_components=R_est, n_epochs=5000)\n",
    "sca_latent=sca.fit_transform(X=fit_data)\n",
    "# sca_latent=sca.fit_transform(X=fit_data, sample_weight=sample_weights)\n",
    "\n",
    "\n",
    "# ##### PCA\n",
    "# pca = WeightedPCA(n_components = R_est)\n",
    "# pca_latent = pca.fit_transform(fit_data,sample_weight=sample_weights)\n",
    "\n",
    "# ##### PCA+Varimax\n",
    "# pca_var = WeightedPCA(n_components = R_est, rotate=True)\n",
    "# pca_var_latent = pca_var.fit_transform(fit_data,sample_weight=sample_weights)\n",
    "\n",
    "# #### Factor Analysis\n",
    "# fa = FactorAnalysis(n_components= R_est)\n",
    "# fa_latent = fa.fit_transform(fit_data)\n",
    "\n",
    "# #### NMF\n",
    "# nmf = Nmf(fit_data_pos,rank=R_est)\n",
    "# nmf_fit = nmf()\n",
    "# nmf_latent=np.array(nmf_fit.basis())\n",
    "\n",
    "#### ICA\n",
    "# ica = FastICA(R_est)\n",
    "ica = FastICA(R_est,whiten='unit-variance')\n",
    "ica_latent=ica.fit_transform(fit_data)\n",
    "\n",
    "#### Sparse NMF\n",
    "snmf = Snmf(fit_data_pos,rank=R_est,version='l')\n",
    "snmf_fit = snmf()\n",
    "snmf_latent=np.array(snmf_fit.basis())\n",
    "\n",
    "#### Sparse PCA\n",
    "# spca = SparsePCA(n_components= R_est)\n",
    "# spca_latent = spca.fit_transform(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snmf = Snmf(fit_data_pos,rank=R_est,version='l',n_run=2)\n",
    "# snmf_fit = snmf()\n",
    "# snmf_latent=np.array(snmf_fit.basis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snmf = Snmf(fit_data_pos,rank=R_est,version='l',beta=1e-3)\n",
    "# snmf_fit = snmf()\n",
    "# snmf_latent=np.array(snmf_fit.basis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snmf_latent=snmf_latent*np.linalg.norm(snmf_fit.coef(),axis=1)[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list that has latents from all comparison methods\n",
    "# latents=[sca_latent,pca_latent,fa_latent,nmf_latent, spca_latent,ica_latent,snmf_latent,pca_var_latent]\n",
    "# latent_names=['SCA','PCA','FA','NMF','SPCA','ICA','SNMF','PCA+Varimax']\n",
    "\n",
    "latents=[sca_latent,ica_latent,snmf_latent]\n",
    "latent_names=['SCA','ICA','SNMF']\n",
    "\n",
    "\n",
    "\n",
    "num_comparisons=len(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape latents\n",
    "\n",
    "rs_latents=[np.reshape(np.array(latents[i]),(-1,8,R_est),order = 'F') for i in range(num_comparisons)]\n",
    "\n",
    "# ### Ordered latents\n",
    "\n",
    "rs_latents_ordered=[]\n",
    "\n",
    "# for rs_latent in rs_latents: \n",
    "\n",
    "#     # calculate across condition variance\n",
    "#     var = np.var(rs_latent,axis = 1)\n",
    "\n",
    "#     # find peak occupancy of each dimension\n",
    "#     pkIdx = np.argmax(var,axis = 0)\n",
    "\n",
    "#     # define plotting order\n",
    "#     order = np.argsort(pkIdx)\n",
    "\n",
    "#     # resort ssa_latents by time of maximum occupancy\n",
    "#     rs_latents_ordered.append(rs_latent[:,:,order]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occThresh_fract=.07\n",
    "\n",
    "rs_sca_latent=rs_latents[0]\n",
    "\n",
    "# calculate across condition variance\n",
    "sca_var = np.var(rs_sca_latent,axis = 1)\n",
    "\n",
    "# calculate max occupancy for each dimension\n",
    "maxOcc_sca = np.max(sca_var, axis=0)\n",
    "\n",
    "# set occupancy threshold\n",
    "occThresh = np.max(maxOcc_sca) * occThresh_fract\n",
    "\n",
    "# initialize a vector for sorting the dimensions and move unoccupied dimensions to the end of the list\n",
    "sca_order = np.zeros(R_est)\n",
    "\n",
    "# detect low occupancy dimensions\n",
    "lowOccDims = np.argwhere(maxOcc_sca < occThresh)[:,0]\n",
    "highOccDims = np.argwhere(maxOcc_sca >= occThresh)[:,0]\n",
    "numLowOcc = lowOccDims.shape[0]\n",
    "numHighOcc = highOccDims.shape[0]\n",
    "\n",
    "# move them to the end of the list\n",
    "sca_order[-numLowOcc:] = lowOccDims\n",
    "\n",
    "# grab the rest of the dimensions and sort them by time of peak occupancy\n",
    "sca_var = sca_var[:,highOccDims]\n",
    "\n",
    "# find peak occupancy of each dimension\n",
    "pkIdx = np.argmax(sca_var,axis = 0)\n",
    "\n",
    "# define plotting order of the high occupancy dimensions and add to list\n",
    "highOrder = np.argsort(pkIdx)\n",
    "sca_order[:numHighOcc] = highOccDims[highOrder]\n",
    "sca_order = np.copy(sca_order.astype(int))\n",
    "\n",
    "# resort ssa_latents by time of maximum occupancy\n",
    "rs_sca_latent = rs_sca_latent[:,:,sca_order]\n",
    "\n",
    "rs_latents_ordered.append(rs_sca_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,rs_latent in enumerate(rs_latents): \n",
    "    if i>0:\n",
    "\n",
    "        # calculate across condition variance\n",
    "        var = np.var(rs_latent,axis = 1)\n",
    "\n",
    "        # find peak occupancy of each dimension\n",
    "        pkIdx = np.argmax(var,axis = 0)\n",
    "\n",
    "        # define plotting order\n",
    "        order = np.argsort(pkIdx)\n",
    "\n",
    "        # resort ssa_latents by time of maximum occupancy\n",
    "        rs_latents_ordered.append(rs_latent[:,:,order]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate max latent values to create y limits for plotting\n",
    "ymaxes=[1.01*np.max(np.abs(latents[i])) for i in range(num_comparisons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMap  = ['#5e0044', '#6f144e', '#812858', '#933c62', '#a5506d', '#b76477', '#c97881', '#db8c8c']\n",
    "\n",
    "# define some useful time points\n",
    "tgt_idx=20\n",
    "move_idx=77\n",
    "ret_idx=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_latents=[0,5,6]\n",
    "plot_latents=[0,1,2]\n",
    "\n",
    "titles=[latent_names[p]for p in plot_latents]\n",
    "\n",
    "num_plotted_latents=len(plot_latents)\n",
    "\n",
    "fig = make_subplots(rows=R_est,cols = num_plotted_latents,shared_xaxes = True,vertical_spacing = 0,subplot_titles=titles)\n",
    "\n",
    "for k,l in enumerate(plot_latents):\n",
    "    for ii in range(R_est):\n",
    "\n",
    "        for jj in range(numConds):\n",
    "            latTrace = go.Scatter(y = rs_latents_ordered[l][:,jj,ii], line = go.scatter.Line(color = cMap[jj],width = 2.5),showlegend = False)\n",
    "            fig.add_trace(latTrace,row = ii+1,col=k+1)\n",
    "\n",
    "        # mark important task events\n",
    "        fig.add_vline(x = tgt_idx,row = ii+1,col = k+1, line_color = 'black')\n",
    "        fig.add_vline(x = move_idx,row = ii+1,col = k+1, line_color = 'black')\n",
    "        fig.add_vline(x = ret_idx,row = ii+1,col = k+1, line_color = 'black')\n",
    "\n",
    "    #clean up\n",
    "        fig.update_yaxes(showgrid = False,zeroline = False,visible = False,range = [-ymaxes[l],ymaxes[l]],row=ii+1,col=k+1)\n",
    "\n",
    "    # clean up\n",
    "    fig.update_layout(height = 2000,width =1500,\n",
    "                      paper_bgcolor = 'white',\n",
    "                      plot_bgcolor = 'white')    \n",
    "\n",
    "\n",
    "    fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,visible = False)\n",
    "    fig.update_xaxes(color = 'black',showgrid = False,zeroline = False,\n",
    "                     ticks = 'outside',tickvals = [0,50],ticktext = ['0','500'],visible = True,row = R_est,col = k+1)\n",
    "\n",
    "# fig.show()\n",
    "figDir='/Users/jig289/Dropbox/SCA/SCA_Stuff/sca_resubmit/figures/'\n",
    "\n",
    "\n",
    "fig.write_image(figDir + monkName + '_CO_50dims_defaultsca_noweight.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
