{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "from scipy import io\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# from ssa_functions import fit_ssa, get_sample_weights, weighted_pca, weighted_rrr\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Documents/Projects/Churchland/Sparsity/code/ssa')\n",
    "\n",
    "from ssa import fit_ssa, weighted_pca\n",
    "from ssa.util import get_sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which monkey are we working with?\n",
    "monkName = 'Balboa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with rates\n",
    "load_folder='/Users/sherryan/sca_data/'\n",
    "\n",
    "# load data\n",
    "data=io.loadmat(load_folder + monkName + '_outAndBack_redYellowConds_rawRates.mat')\n",
    "\n",
    "# pull out the psths\n",
    "# data is a C x N x T tensor \n",
    "data_array=data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsample data (using a factor of 10 here)\n",
    "data_downsamp=data_array[:,:,np.arange(0,data_array.shape[2],10)]\n",
    "\n",
    "# pull out some useful numbers\n",
    "numConds,numN,trlDur = np.shape(data_downsamp)\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_concat=data_downsamp.swapaxes(0,1).reshape([data_downsamp.shape[1],data_downsamp.shape[0]*data_downsamp.shape[2]])\n",
    "\n",
    "#fr range\n",
    "fr_range=np.ptp(data_concat,axis=1)[:,None]\n",
    "\n",
    "# make a time mask\n",
    "timeMask = np.tile(np.arange(trlDur),(1,numConds)).T.flatten()\n",
    "\n",
    "# define the times we want to use for ssa/pca\n",
    "# target on: 20\n",
    "# move on:   77\n",
    "# return:    200\n",
    "trainTimes = np.arange(20,230)\n",
    "\n",
    "# define a 'training mask' for convenience \n",
    "trainMask = np.in1d(timeMask,trainTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract cross-condition mean\n",
    "data_scm=data_downsamp-np.mean(data_downsamp,axis=0)[None,:,:]\n",
    "\n",
    "#Concatenate all the conditions (so the matrix is size N x TC instead of C x N x T)\n",
    "data_scm_concat=data_scm.swapaxes(0,1).reshape([data_downsamp.shape[1],data_downsamp.shape[0]*data_downsamp.shape[2]])\n",
    "\n",
    "#Soft normalize (divide each neuron by its fr range + 5)\n",
    "data_scm_norm=data_scm_concat/(fr_range+5)\n",
    "\n",
    "# mean-center the data\n",
    "dMean = np.tile(np.mean(data_scm_norm,axis=1)[:,np.newaxis],(1,data_scm_norm.shape[1]))\n",
    "data_mc = data_scm_norm - dMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose magnitude for sparcity penalty \n",
    "- this is a single free parameter that tells ssa how much to prioritize sparsity (relative to reconstruction error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose data and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the data for convenienc\n",
    "#Note that model requires (T x N) input rather than (N x T), which is why there are transposes below\n",
    "fit_data=np.copy(data_mc.T) \n",
    "\n",
    "# how much to weight each timestep (used by)\n",
    "sample_weights=get_sample_weights(fit_data)\n",
    "\n",
    "# number of dimensions to find\n",
    "R_est=8\n",
    "\n",
    "# range of lambda values to test\n",
    "lambdaRange = np.logspace(-5,1,num = 10)\n",
    "\n",
    "# number of tested lambdas\n",
    "numLambda = lambdaRange.shape[0]\n",
    "\n",
    "#Number of epochs of model fitting\n",
    "numEpochs=3000\n",
    "\n",
    "#Learning rate of model fitting\n",
    "learningRate=.001\n",
    "\n",
    "#Whether to print the model error while fitting\n",
    "vBose=True\n",
    "\n",
    "# Whether or not to impose hard orthogonality constraint \n",
    "hardOrthFlag = False\n",
    "\n",
    "# Soft orthogonality penalty\n",
    "# note that requiring a hard orthogonality constraint greatly slows down SSA, especially on large datasets. Having a soft penalty speeds everything up by about a factor of 10 and finds orthogonal dimensions. \n",
    "lam_orth = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through sparcity lambdas\n",
    "X_pred = np.zeros((fit_data.shape[0],fit_data.shape[1],numLambda))\n",
    "\n",
    "# cycle through lambdas\n",
    "for L in np.arange(numLambda):\n",
    "\n",
    "    # fit model\n",
    "    model,latent, x_pred,losses=fit_ssa(X=fit_data,sample_weight = sample_weights,\n",
    "                                   R=R_est,lam_sparse = lambdaRange[L],lr=learningRate,n_epochs=numEpochs,\n",
    "                                   orth=hardOrthFlag, lam_orthog=lam_orth)\n",
    "\n",
    "    # pull out the latents and the reconstructed data \n",
    "    x_pred = x_pred.detach().numpy()\n",
    "\n",
    "    # save the reconstructions\n",
    "    X_pred[:,:,L] = x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reconstructions from above \n",
    "saveDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/data/sparsityLambda_tests/'\n",
    "np.savez(saveDir+ monkName + '_' + 'centerOutReaching_sparsityLambdaTest'\n",
    "           , predData = X_pred, data = fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run weighted PCA for comparison reconstruction error\n",
    "\n",
    "# run pca\n",
    "U_est,V_est = weighted_pca(fit_data[trainMask,:],R_est,sample_weights[trainMask])\n",
    "\n",
    "# get latents\n",
    "pca_latent = fit_data@U_est\n",
    "\n",
    "# reconstruct neurons\n",
    "reconData_pca = np.reshape(pca_latent @ V_est,(-1,1),order = 'F')\n",
    "\n",
    "# calculate reconstruction error \n",
    "oRates = fit_data.reshape(-1,1,order = 'F')\n",
    "\n",
    "reconError_pca = np.sum( (oRates - reconData_pca)**2, axis = 0)\n",
    "\n",
    "# calculate reconstruction error from U.T, rather than V\n",
    "# reconstruct neurons\n",
    "reconData_pca = np.reshape(pca_latent @ U_est.T,(-1,1),order = 'F')\n",
    "\n",
    "# calculate reconstruction error \n",
    "oRates = fit_data.reshape(-1,1,order = 'F')\n",
    "\n",
    "reconError_pca_U = np.sum( (oRates - reconData_pca)**2, axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change the default label color to white \n",
    "plt.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "\n",
    "\n",
    "# plot the reconstruction errors calculated above as a function of sparsity lambda \n",
    "\n",
    "# reshape reconstructed population from size CT x N x K to CTN x K\n",
    "reconData = X_pred[:,:,:-1].reshape(-1,numLambda-1,order = 'F')\n",
    "\n",
    "# reshape original data to CTN x 1 vectors\n",
    "oRates = fit_data.reshape(-1,1, order = 'F')\n",
    "\n",
    "# calculate total error between reconstruction and original rates\n",
    "reconError = np.sum( (oRates - reconData)**2, axis = 0)\n",
    "\n",
    "# plot error\n",
    "plt.figure(figsize = (5,5));\n",
    "plt.plot(lambdaRange[0:-1], reconError, color = 'r',label = 'reconstruction error');\n",
    "\n",
    "# change x scale to a logscale\n",
    "plt.xscale('log')\n",
    "\n",
    "# add labels\n",
    "plt.xlabel('sparsity penalty')\n",
    "plt.ylabel('reconstruction error')\n",
    "\n",
    "    \n",
    "# mark the default lambda penalty for this dataset\n",
    "plt.vlines(x = 0.015,ymin = 2000,ymax = 5000,color = 'k',label = 'default lam_sparse for this dataset');\n",
    "\n",
    "# mark pca reconstruction error\n",
    "plt.hlines(y = reconError_pca,xmin = 10e-5,xmax = 1,color = 'b',linestyle = '--',label = 'pca reconstruction error');\n",
    "\n",
    "\n",
    "# add a legend\n",
    "legend = plt.legend();\n",
    "# change the font color so we can see the legend\n",
    "for text in legend.get_texts():\n",
    "    text.set_color('black');\n",
    "    \n",
    "# save figure\n",
    "# save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_reconstructionError.pdf',dpi = 'figure')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how does ability to reconstruct activity change with R_est? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the data for convenienc\n",
    "#Note that model requires (T x N) input rather than (N x T), which is why there are transposes below\n",
    "fit_data=np.copy(data_scm_norm.T) \n",
    "\n",
    "# how much to weight each timestep (used by)\n",
    "sample_weights=get_sample_weights(fit_data)\n",
    "\n",
    "# number of dimensions to find\n",
    "R_est=np.arange(2,25)\n",
    "numR = R_est.shape[0]\n",
    "\n",
    "# sparcity penalty\n",
    "sLambda= 0.01\n",
    "\n",
    "#Number of epochs of model fitting\n",
    "numEpochs=3000\n",
    "\n",
    "#Learning rate of model fitting\n",
    "learningRate=.001\n",
    "\n",
    "#Whether to print the model error while fitting\n",
    "vBose=True\n",
    "\n",
    "# Whether or not to impose hard orthogonality constraint \n",
    "hardOrthFlag = False\n",
    "\n",
    "# Soft orthogonality penalty\n",
    "# note that requiring a hard orthogonality constraint greatly slows down SSA, especially on large datasets. Having a soft penalty speeds everything up by about a factor of 10 and finds orthogonal dimensions. \n",
    "lam_orth = 10\n",
    "\n",
    "# generate a condition mask\n",
    "condMask = np.tile(np.arange(numConds),(trlDur,1)).reshape(-1,1,order = 'F').flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through sparcity lambdas\n",
    "X_pred = np.zeros((fit_data.shape[0],fit_data.shape[1],numR))\n",
    "minMax_var = np.zeros((numR,1))\n",
    "\n",
    "# cycle through lambdas\n",
    "for R in np.arange(numR):\n",
    "\n",
    "    # fit model\n",
    "    model,latent, x_pred=fit_ssa(X=fit_data,sample_weight = sample_weights,\n",
    "                                   R=R_est[R],lam = sLambda,lr=learningRate,n_epochs=numEpochs,\n",
    "                                   orth=hardOrthFlag, lam2=lam_orth,verbose=vBose)\n",
    "\n",
    "    # pull out the latents and the reconstructed data \n",
    "    x_pred = x_pred.detach().numpy()\n",
    "\n",
    "    # grab the latents\n",
    "    latent=latent.detach().numpy()\n",
    "\n",
    "    # reshape to be size T x C x K\n",
    "    latent = np.reshape(latent,[-1,8,R_est[R]],order = 'F');\n",
    "\n",
    "    # calculate the minimum (across dimensions) maximum (across time) cross-condition variance \n",
    "    minMax_var[R] = np.min(np.max(np.var(latent,axis = 1)))\n",
    "\n",
    "    # save the reconstructions\n",
    "    X_pred[:,:,R] = x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reconstructions from above \n",
    "saveDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/data/reaching/'\n",
    "np.savez(saveDir+ monkName + '_' + 'centerOutReaching_numDimsTest'\n",
    "           , predData = X_pred, data = fit_data, minMax_ccVar = minMax_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the default label color to white \n",
    "plt.rcParams['text.color'] = 'w'\n",
    "plt.rcParams['xtick.color'] = 'w'\n",
    "plt.rcParams['ytick.color'] = 'w'\n",
    "plt.rcParams['axes.labelcolor'] = 'w'\n",
    "\n",
    "\n",
    "# plot the reconstruction errors calculated above as a function of sparsity lambda \n",
    "\n",
    "# reshape reconstructed population from size CT x N x K to CTN x K\n",
    "reconData = X_pred.reshape(-1,numR,order = 'F')\n",
    "\n",
    "# reshape original data to CTN x 1 vectors\n",
    "oRates = fit_data.reshape(-1,1, order = 'F')\n",
    "\n",
    "# calculate total error between reconstruction and original rates\n",
    "reconError = np.sum( (oRates - reconData)**2, axis = 0)\n",
    "\n",
    "# plot error\n",
    "plt.figure(figsize = (5,5));\n",
    "plt.plot(R_est, reconError, color = 'r',label = 'reconstruction error');\n",
    "\n",
    "# add labels\n",
    "plt.xlabel('number of dimensions')\n",
    "plt.ylabel('reconstruction error')\n",
    "\n",
    "# add a legend\n",
    "legend = plt.legend();\n",
    "# change the font color so we can see the legend\n",
    "for text in legend.get_texts():\n",
    "    text.set_color('black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit ssa with the chosen sparsity penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "sparsityLambda = 0.01\n",
    "R_est = 8\n",
    "model,ssa_latent, x_pred,losses=fit_ssa(X=fit_data[trainMask,:],sample_weight = sample_weights[trainMask],\n",
    "                                R=R_est,lam_sparse = None,lr=learningRate,n_epochs=numEpochs*2,\n",
    "                                orth=hardOrthFlag, lam_orthog=lam_orth)\n",
    "\n",
    "# grab weights\n",
    "ssaW = model.fc1.weight.detach().numpy()\n",
    "\n",
    "# project all of the data into the ssa dimensions\n",
    "ssa_latent = fit_data @ ssaW.T\n",
    "\n",
    "# calculate reconstruction R2\n",
    "X_hat = x_pred.detach().numpy()\n",
    "X = fit_data[trainMask,:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate reconstruction R2\n",
    "X_hat = x_pred.detach().numpy().reshape(-1,1,order = 'F')\n",
    "X = fit_data[trainMask,:].reshape(-1,1,order='F')\n",
    "\n",
    "SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "R2_ssa = 1 - (SS_res/SS_tot)\n",
    "\n",
    "# display results\n",
    "print('SSA R2: ' + str(R2_ssa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot product of the learned U weights (which are not constrained to be orthogonal)\n",
    "plt.imshow(model.fc1.weight.detach().numpy()@model.fc1.weight.detach().numpy().T,clim=[-0.1,0.1],cmap='RdBu');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dot product of the learned V weights (which may or may not be constrained to be orthogonal, depending on the 'orthFlg') \n",
    "plt.imshow(model.fc2.weight.detach().numpy().T@model.fc2.weight.detach().numpy(),clim=[-0.1,0.1],cmap='RdBu');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit PCA for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_est,V_est = weighted_pca(fit_data[trainMask,:],R_est,sample_weights[trainMask])\n",
    "pca_latent = fit_data@U_est\n",
    "\n",
    "# calculate reconstruction R2\n",
    "X = fit_data[trainMask,:]\n",
    "X_hat = X @U_est @ V_est\n",
    "X_hat = X_hat.reshape(-1,1,order = 'F')\n",
    "X = X.reshape(-1,1,order = 'F')\n",
    "\n",
    "SS_tot = np.sum( (X - np.mean(X))**2)\n",
    "SS_res = np.sum( (X - X_hat)**2)\n",
    "\n",
    "R2_pca = 1 - (SS_res/SS_tot)\n",
    "\n",
    "# display results\n",
    "print('PCA R2: ' + str(R2_pca))\n",
    "\n",
    "def get_sses_pred(y_test,y_test_pred):\n",
    "    sse=np.sum((y_test_pred-y_test)**2,axis=0)\n",
    "    return sse\n",
    "\n",
    "def get_sses_mean(y_test):\n",
    "    y_mean=np.mean(y_test,axis=0)\n",
    "    sse_mean=np.sum((y_test-y_mean)**2,axis=0)\n",
    "    return sse_mean\n",
    "sses=get_sses_pred(X,X_hat)\n",
    "sses_mean=get_sses_mean(X)\n",
    "print(1-np.sum(sses)/np.sum(sses_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ssa and pca latents, calculate mean (and std) pairwise correlation \n",
    "\n",
    "# ssa first\n",
    "ssa_rho = np.abs(np.corrcoef(ssa_latent.T))\n",
    "\n",
    "# only keep lower triangle\n",
    "ssa_rho = ssa_rho[np.triu_indices(R_est,k=1)]\n",
    "\n",
    "# pca\n",
    "pca_rho = np.abs(np.corrcoef(pca_latent.T))\n",
    "pca_rho = pca_rho[np.triu_indices(R_est,k=1)]\n",
    "\n",
    "# calculate mean (and std) of pairwise correlations\n",
    "mPCA = np.mean(pca_rho)\n",
    "stdPCA = np.std(pca_rho)\n",
    "\n",
    "mSSA = np.mean(ssa_rho)\n",
    "stdSSA = np.std(ssa_rho)\n",
    "\n",
    "print('mPCA (std) ' + str(mPCA) + '(' + str(stdPCA) + ')' )\n",
    "print('mSSA (std) ' + str(mSSA) + '(' + str(stdSSA) + ')' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate variance accounted for by pca and ssa latents\n",
    "\n",
    "# total variance\n",
    "totVar = np.sum(np.var(fit_data,axis=0))\n",
    "\n",
    "# variance of pca latents\n",
    "pcaVar = np.var(pca_latent,axis=0)\n",
    "\n",
    "# variance of ssa latents (ordered by variance explained)\n",
    "ssaVar = np.sort(np.var(ssa_latent, axis = 0))[::-1]\n",
    "\n",
    "# cumulative sum of fraction of variance explained\n",
    "cumSum_pca = np.cumsum(pcaVar) / totVar\n",
    "cumSum_ssa = np.cumsum(ssaVar) / totVar\n",
    "\n",
    "# plot\n",
    "plt.plot(cumSum_pca,linewidth = 2,color = 'r')\n",
    "plt.plot(cumSum_ssa,linewidth = 2,color = 'k')\n",
    "\n",
    "# ratio of variance explained\n",
    "print('ratio of variance explained: ' + str(np.sum(cumSum_ssa)/ np.sum(cumSum_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some plotting colors\n",
    " - ssa is purple\n",
    " - pca is washed out purple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of colormap to use\n",
    "colorIdx = np.arange(0.55,1,0.4/8)\n",
    "\n",
    "# define ssa colors\n",
    "ssa_cMap = sns.cubehelix_palette(start = 0.1,rot = 0.6,dark = 0.15, light = 0.8,as_cmap = True,gamma = 1.1)\n",
    "ssa_cMap = ssa_cMap(colorIdx)\n",
    "\n",
    "# define pca colors\n",
    "pca_cMap = sns.cubehelix_palette(start = 0.1,rot = 0.6,dark = 0.15, light = 0.8,as_cmap = True,gamma = 0.7,hue = 0.3)\n",
    "pca_cMap = pca_cMap(colorIdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(8):\n",
    "    plt.plot(np.arange(0,10)*i,color = ssa_cMap[i,:],linewidth = 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(8):\n",
    "    plt.plot(np.arange(0,10)*i,color = pca_cMap[i,:],linewidth = 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot when reordering by time of maximal influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate across-condition variance of each projection as a function of time\n",
    "\n",
    "# reshape both latents to be size T x C x K \n",
    "rs_ssa_latent = np.reshape(ssa_latent,(-1,8,R_est),order = 'F')\n",
    "\n",
    "# calcualte across condition variance\n",
    "ssa_var = np.var(rs_ssa_latent,axis = 1)\n",
    "\n",
    "# find peak occupancy of each dimension\n",
    "pkIdx = np.argmax(ssa_var,axis = 0)\n",
    "\n",
    "# define plotting order\n",
    "ssa_order = np.argsort(pkIdx)\n",
    "\n",
    "# do the same for pca\n",
    "rs_pca_latent = np.reshape(pca_latent,(-1,8,R_est),order = 'F')\n",
    "pca_var = np.var(rs_pca_latent,axis = 1)\n",
    "pkIdx = np.argmax(pca_var,axis = 0)\n",
    "\n",
    "# define plotting order\n",
    "pca_order = np.argsort(pkIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of each trial\n",
    "T=data_downsamp.shape[2] #Length of time per condition\n",
    "trs=np.arange(0,8)\n",
    "t_idxs=[np.arange(T*tr,T*(tr+1)) for tr in trs]\n",
    "\n",
    "# define some useful time points\n",
    "tgt_idx=20\n",
    "move_idx=77\n",
    "ret_idx=200\n",
    "\n",
    "# change color of axis labels so we can see them in the pdf\n",
    "plt.rcParams['text.color'] = 'w'\n",
    "plt.rcParams['xtick.color'] = 'w'\n",
    "plt.rcParams['ytick.color'] = 'w'\n",
    "plt.rcParams['axes.labelcolor'] = 'w'\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(R_est,1,figsize=(15,20))\n",
    "for i in range(R_est):\n",
    "    for j in range(len(trs)):\n",
    "\n",
    "        ax[i].plot(pca_latent[:,pca_order[i]][t_idxs[j]] - np.mean(pca_latent[:,pca_order[i]]),linewidth=2.25,color=pca_cMap[j,:])\n",
    "        \n",
    "        ax[i].plot([tgt_idx,tgt_idx],[-1.7,1.7],'gray',linewidth=.5)\n",
    "        ax[i].plot([move_idx,move_idx],[-2,2],'k',linewidth=.5)\n",
    "        ax[i].plot([ret_idx,ret_idx],[-2,2],'k',linewidth=.5)\n",
    "\n",
    "        ax[i].set_xlim([0,T+1])\n",
    "        ax[i].set_ylim([-2.5, 2.5])\n",
    "\n",
    "        if i<R_est-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time (10ms bins)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Weighted PCA')\n",
    "\n",
    "# save figure\n",
    "\n",
    "# save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_noMean_sub_pcaProj.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of each trial\n",
    "T=data_downsamp.shape[2] #Length of time per condition\n",
    "trs=np.arange(0,8)\n",
    "t_idxs=[np.arange(T*tr,T*(tr+1)) for tr in trs]\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(R_est,1,figsize=(15,20))\n",
    "for i in range(R_est):\n",
    "    for j in range(len(trs)):\n",
    "\n",
    "        ax[i].plot(ssa_latent[:,ssa_order[i]][t_idxs[j]] - np.mean(ssa_latent[:,ssa_order[i]]),linewidth=2.25,color=ssa_cMap[j,:])\n",
    "        \n",
    "        ax[i].plot([tgt_idx,tgt_idx],[-1.7,1.7],'gray',linewidth=.5)\n",
    "        ax[i].plot([move_idx,move_idx],[-2,2],'k',linewidth=.5)\n",
    "        ax[i].plot([ret_idx,ret_idx],[-2,2],'k',linewidth=.5)\n",
    "        ax[i].plot([0,T],[0,0],'k--')\n",
    "\n",
    "        ax[i].set_xlim([0,T+1])\n",
    "        ax[i].set_ylim([-2.5, 2.5])\n",
    "\n",
    "        if i<R_est-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time (10ms bins)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('SSA')\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_noMeanSub_ssaProj.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the across-condition variance at three time points: preparation, hold, movement\n",
    "- target on: 20\n",
    "- outward movement: 77\n",
    "- return movement: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the time points we care about\n",
    "prepWindow = np.arange(22,52)\n",
    "moveWindow = np.arange(195,225)\n",
    "holdWindow = np.arange(130,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the across-condition variance within each window, for each ssa and pca dimension\n",
    "\n",
    "# initialize K x 3 array to hold results\n",
    "ssaVar = np.zeros((R_est,3))\n",
    "pcaVar = np.zeros((R_est,3))\n",
    "\n",
    "# number of conditions\n",
    "numConds = trs.shape[0]\n",
    "\n",
    "# cycle through dimensions\n",
    "for d in np.arange(R_est):\n",
    "\n",
    "    # grab and reshape our projections to be size t x c\n",
    "    ssaProj = ssa_latent[:,d].reshape([-1,numConds],order = 'F')\n",
    "    pcaProj = pca_latent[:,d].reshape([-1,numConds],order = 'F')\n",
    "\n",
    "\n",
    "    # calculate across-condition variance \n",
    "    projVar_ssa = np.var(ssaProj,axis = 1)\n",
    "    projVar_pca = np.var(pcaProj,axis = 1)\n",
    "\n",
    "    # normalize variance by total cross-condtion variance in this dimension\n",
    "    totVar_ssa= np.sum(projVar_ssa)\n",
    "    totVar_pca= np.sum(projVar_pca)\n",
    "\n",
    "    # prep variance\n",
    "    ssaVar[d,0] = np.sum(projVar_ssa[prepWindow]) / totVar_ssa\n",
    "    pcaVar[d,0] = np.sum(projVar_pca[prepWindow]) / totVar_pca\n",
    "\n",
    "    # move variance\n",
    "    ssaVar[d,1] = np.sum(projVar_ssa[moveWindow]) / totVar_ssa\n",
    "    pcaVar[d,1] = np.sum(projVar_pca[moveWindow]) / totVar_pca\n",
    "\n",
    "    # hold variance\n",
    "    ssaVar[d,2] = np.sum(projVar_ssa[holdWindow]) / totVar_ssa\n",
    "    pcaVar[d,2] = np.sum(projVar_pca[holdWindow]) / totVar_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot variances \n",
    "- prep vs. move\n",
    "- move vs. hold\n",
    "- hold vs. prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plotting colors\n",
    "pcaColor = 'lightslategray'\n",
    "ssaColor = 'purple'\n",
    "\n",
    "# set up figure\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 3,figsize = (8,5))\n",
    "\n",
    "# prep vs. move\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(ssaVar[:,1],ssaVar[:,0],'o',color = ssaColor,mfc = ssaColor,ms = 10 )\n",
    "plt.plot(pcaVar[:,1],pcaVar[:,0],'o',color = pcaColor,mfc = pcaColor,ms = 10 )\n",
    "plt.xlim([-0.05,0.5]);plt.ylim([-0.05,0.5]);\n",
    "plt.xlabel('move');plt.ylabel('prep');\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# move vs. hold\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(ssaVar[:,2],ssaVar[:,1],'o',color = ssaColor,mfc = ssaColor,ms = 10 )\n",
    "plt.plot(pcaVar[:,2],pcaVar[:,1],'o',color = pcaColor,mfc = pcaColor,ms = 10 )\n",
    "plt.xlim([-0.05,0.5]);plt.ylim([-0.05,0.5]);\n",
    "plt.xlabel('hold');plt.ylabel('move');\n",
    "\n",
    "\n",
    "# hold vs. prep\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(ssaVar[:,0],ssaVar[:,2],'o',color = ssaColor,mfc = ssaColor,ms = 10 )\n",
    "plt.plot(pcaVar[:,0],pcaVar[:,2],'o',color = pcaColor,mfc = pcaColor,ms = 10 )\n",
    "plt.xlim([-0.05,0.5]);plt.ylim([-0.05,0.5]);\n",
    "plt.xlabel('prep');plt.ylabel('hold');\n",
    "\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap neurons, run weighted pca, generate a bunch of loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define a function to feed to the parallel pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPCA_reaching(counter):\n",
    "    \n",
    "    # X is a matrix of size CT x N \n",
    "    dataL,numN = fit_data.shape\n",
    "    \n",
    "    # draw 'numN' neurons with replacement\n",
    "    nIdx = np.random.choice(numN,numN)\n",
    "    X_samp = fit_data[:,nIdx]\n",
    "    \n",
    "    # calculate our sample weights\n",
    "    sWeights = get_sample_weights(X_samp)\n",
    "     \n",
    "    # run weighted pca\n",
    "    uEst,vEst = weighted_pca(X_samp,R_est,sWeights)\n",
    "    \n",
    "    # pca latents\n",
    "    lat = X_samp @ uEst\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parallel pool\n",
    "# 'Pool' defaults to the number of available CPUs\n",
    "pool = Pool()\n",
    "\n",
    "# fit pca \n",
    "out = pool.imap_unordered(runPCA_reaching,np.arange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out all of our loadings\n",
    "pca_bs_L = [*out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load projections into prep, move, and posture space \n",
    "- calculated via gamal's orthogonal dim red. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with rates\n",
    "load_folder='/Users/andrew/Documents/Projects/Churchland/Sparsity/data/reaching/'\n",
    "\n",
    "# load data\n",
    "data=io.loadmat(load_folder + monkName + '_gamalLoadings.mat')\n",
    "\n",
    "# pull out prep, move, and posture projections\n",
    "# (N x k)\n",
    "gPrepProj=data['prepProj'][:,:,None]\n",
    "gMoveProj=data['moveProj'][:,:,None]\n",
    "gPostProj=data['postProj'][:,:,None]\n",
    "\n",
    "# concatenate all projections\n",
    "gProj = np.concatenate((gPrepProj,gMoveProj,gPostProj),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gPrepProj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regress pca and ssa latents against prep, move, or posture dimensions\n",
    "- ask how well the prep, move, or posture projections (ground truth) can reconstruct ssa/pca projection in a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "numBoots = len(pca_bs_L)\n",
    "\n",
    "# initialize vector to hold R2\n",
    "# each column corresponds to the reconstruction from the prep, move, or posture projections\n",
    "allPCA_R2 = np.zeros((numBoots,R_est,3))\n",
    "\n",
    "# cycle through bootstrap repetitions\n",
    "for ii in np.arange(numBoots):\n",
    "\n",
    "    # pull out bootstrap latents\n",
    "    Y = pca_bs_L[ii]\n",
    "\n",
    "    # get B (regression weights) from prep, move, and posture projections\n",
    "    for e in np.arange(3):\n",
    "\n",
    "        # make life a bit easier and just pull out the gamal projections we want\n",
    "        X = gProj[:,:,e]\n",
    "\n",
    "        # regress\n",
    "        B = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "        # reconstruct Y \n",
    "        Y_hat = X @ B\n",
    "    \n",
    "        # calculate R2\n",
    "        ss_tot = np.sum((Y - np.mean(Y,axis = 0))**2, axis = 0)\n",
    "        ss_res = np.sum((Y - Y_hat)**2,axis = 0)\n",
    "        allPCA_R2[ii,:,e] = 1 - (ss_res/ss_tot)\n",
    "\n",
    "# take max R2 across each epoch \n",
    "maxR2_pca = np.max(allPCA_R2,axis = 2)\n",
    "\n",
    "# redo regression analysis for ssa projections\n",
    "ssa_R2 = np.zeros((1,R_est,3))\n",
    "\n",
    "# rename some variables for consistency\n",
    "Y = ssa_latent\n",
    "\n",
    "for e in np.arange(3):\n",
    "\n",
    "    # make life a bit easier and just pull out the gamal projections we want\n",
    "    X = gProj[:,:,e]\n",
    "\n",
    "    # regress\n",
    "    B = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "    # reconstruct Y \n",
    "    Y_hat = X @ B\n",
    "    \n",
    "    # calculate R2\n",
    "    ss_tot = np.sum((Y - np.mean(Y,axis = 0))**2, axis = 0)\n",
    "    ss_res = np.sum((Y - Y_hat)**2,axis = 0)\n",
    "    ssa_R2[0,:,e] = 1 - (ss_res/ss_tot)\n",
    "\n",
    "# take max across epochs\n",
    "maxR2_ssa = np.max(ssa_R2,axis = 2).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change color of axis labels so we can see them in the pdf\n",
    "plt.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "\n",
    "# reshape pca reconstructions\n",
    "pcaR2_recon = np.reshape(maxR2_pca,[-1,1],order = 'F')\n",
    "\n",
    "\n",
    "# plot pca results with a small amount of jitter around 1\n",
    "pcaLoc = np.random.normal(loc = 1,scale = 0.03,size = (numBoots*R_est,1))\n",
    "\n",
    "# plot means\n",
    "plt.plot(pcaLoc,pcaR2_recon,'o',color = (0.4,0.4,0.4),ms = 4,alpha = 0.01);\n",
    "\n",
    "# plot the mean and std of pca performance\n",
    "plt.errorbar(1,np.mean(pcaR2_recon),np.std(pcaR2_recon),color = 'k',lw = 3,zorder = 3)\n",
    "plt.plot(1,np.mean(pcaR2_recon),'o',color = 'k',ms = 8,zorder = 3);\n",
    "\n",
    "\n",
    "# get jittered position for ssa reconstructions\n",
    "ssaLoc = np.random.normal(loc = 2,scale = 0.01,size = (R_est,1))\n",
    "plt.plot(ssaLoc,maxR2_ssa,'o',color = 'purple',alpha = 0.5);\n",
    "\n",
    "# plot the mean and std of ssa performance\n",
    "plt.errorbar(2,np.mean(maxR2_ssa),np.std(maxR2_ssa) / np.sqrt(maxR2_ssa.shape[0]) ,color = 'purple',lw = 3)\n",
    "plt.plot(2,np.mean(maxR2_ssa),'o',color = 'purple',ms = 8);\n",
    "\n",
    "\n",
    "\n",
    "# clean up\n",
    "plt.xlim((0.5, 2.5));plt.ylim((0,1));\n",
    "plt.xticks(np.array([1,2]),('PCA','SSA'));\n",
    "plt.yticks(np.array([0,0.5, 1]));\n",
    "plt.title(monkName);\n",
    "plt.ylabel('R2');\n",
    "\n",
    "\n",
    "# # save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_ssaVsPCA_reconError.pdf',dpi = 'figure')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of each trial\n",
    "T=data_downsamp.shape[2] #Length of time per condition\n",
    "trs=np.arange(0,8)\n",
    "t_idxs=[np.arange(T*tr,T*(tr+1)) for tr in trs]\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(R_est,1,figsize=(10,20))\n",
    "for i in range(R_est):\n",
    "    for j in range(len(trs)):\n",
    "\n",
    "        ax[i].plot(ssa_latent[:,i][t_idxs[j]],linewidth=2.25,color=ssa_cMap[j,:])\n",
    "        \n",
    "        ax[i].plot([tgt_idx,tgt_idx],[-1.7,1.7],'gray',linewidth=.5)\n",
    "        ax[i].plot([move_idx,move_idx],[-2,2],'k',linewidth=.5)\n",
    "        ax[i].plot([ret_idx,ret_idx],[-2,2],'k',linewidth=.5)\n",
    "        ax[i].plot([0,T],[0,0],'k--')\n",
    "\n",
    "        ax[i].set_xlim([0,T+1])\n",
    "        ax[i].set_ylim([-2, 2])\n",
    "\n",
    "        if i<R_est-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time (10ms bins)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('SSA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot kinematics for one example condition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kinematic data \n",
    "# correct folder\n",
    "load_folder='/Users/andrew/Documents/Projects/Churchland/Sparsity/data/rawRates/'\n",
    "\n",
    "# load data\n",
    "data=io.loadmat(load_folder + 'Balboa' + '_outAndBack_redYellowConds_kinematics.mat')\n",
    "\n",
    "# pull out the psths\n",
    "# position data is 2 (X and Y) x T x N (single neuron recordings)\n",
    "# the condition ID is saved in the same file \n",
    "position= data['allPos']\n",
    "speed =   data['allSpeed']\n",
    "\n",
    "# take average across neurons\n",
    "mPos = np.mean(position,axis = 2)\n",
    "mSpeed = np.mean(speed,axis = 1)\n",
    "\n",
    "# downsample both to match resolution of neural data\n",
    "mPos = mPos[:,np.arange(0,mPos.shape[1],10)]\n",
    "mSpeed = mSpeed[np.arange(0,mSpeed.shape[0],10)]\n",
    "\n",
    "# calculate distance from start as a function of time \n",
    "startPos = np.mean(mPos[:,np.arange(5)],axis = 1)\n",
    "posDiff  = startPos[:,None] -mPos\n",
    "dist     = np.linalg.norm(posDiff,axis = 0)\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize = (8,5));\n",
    "plt.subplot(211);\n",
    "plt.plot(dist,color = 'k',lw = 2,label = 'distance from center');plt.ylim([-5,250]);\n",
    "legend = plt.legend();\n",
    "plt.setp(legend.get_texts(),color = 'k')\n",
    "plt.subplot(212);plt.ylim([-5,125]);\n",
    "plt.plot(mSpeed,color = 'slategrey',lw=2,label = 'speed');\n",
    "legend = plt.legend();\n",
    "plt.setp(legend.get_texts(),color = 'k')\n",
    "\n",
    "\n",
    "# define some useful time points\n",
    "tgt_idx=20\n",
    "move_idx=77\n",
    "ret_idx=200\n",
    "\n",
    "plt.plot([tgt_idx,tgt_idx],[0,120],'k',linewidth=.5);\n",
    "plt.plot([move_idx,move_idx],[0,120],'k',linewidth=.5);\n",
    "plt.plot([ret_idx,ret_idx],[0,120],'k',linewidth=.5);\n",
    "\n",
    "# save\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + 'Balboa' + '_kinematics.pdf',dpi = 'figure')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot gamal projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define some new colormaps\n",
    "# fraction of colormap to use\n",
    "colorIdx = np.arange(0.55,1,0.4/8)\n",
    "\n",
    "# define prep colors\n",
    "prep_cMap = sns.cubehelix_palette(start = 2,rot = 0,dark = 0.15, light = 0.8,as_cmap = True,gamma = 1.1)\n",
    "prep_cMap = prep_cMap(colorIdx)\n",
    "\n",
    "# move colors\n",
    "move_cMap = sns.cubehelix_palette(start = 2.35,rot = 0,dark = 0.15, light = 0.8,as_cmap = True,gamma = 0.7)\n",
    "move_cMap = move_cMap(colorIdx)\n",
    "\n",
    "# posture colors\n",
    "post_cMap = sns.cubehelix_palette(start = 2.8,rot = 0,dark = 0.15, light = 0.8,as_cmap = True,gamma = 0.7)\n",
    "post_cMap = post_cMap(colorIdx)\n",
    "\n",
    "# plot colors\n",
    "plt.figure(figsize = (5,5));\n",
    "plt.subplot(311);\n",
    "for i in np.arange(8):\n",
    "    plt.plot(np.arange(0,10)*i,color = prep_cMap[i,:],linewidth = 8);\n",
    "\n",
    "plt.subplot(312);\n",
    "for i in np.arange(8):\n",
    "    plt.plot(np.arange(0,10)*i,color = move_cMap[i,:],linewidth = 8);\n",
    "\n",
    "plt.subplot(313);\n",
    "for i in np.arange(8):\n",
    "    plt.plot(np.arange(0,10)*i,color = post_cMap[i,:],linewidth = 8);\n",
    "\n",
    "# concatenate colormaps\n",
    "prep_cMap = prep_cMap[:,:,None]\n",
    "move_cMap = move_cMap[:,:,None]\n",
    "post_cMap = post_cMap[:,:,None]\n",
    "\n",
    "g_cMap = np.concatenate((prep_cMap, move_cMap, post_cMap),axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "#Get indices of each trial\n",
    "T=data_downsamp.shape[2] #Length of time per condition\n",
    "trs=np.arange(0,8)\n",
    "t_idxs=[np.arange(T*tr,T*(tr+1)) for tr in trs]\n",
    "\n",
    "# define some useful time points\n",
    "tgt_idx=20\n",
    "move_idx=77\n",
    "ret_idx=200\n",
    "\n",
    "# change color of axis labels so we can see them in the pdf\n",
    "plt.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(R_est,3,figsize=(15,20))\n",
    "for E in range(3):\n",
    "\n",
    "    for i in range(R_est):\n",
    "        for j in range(len(trs)):\n",
    "\n",
    "            \n",
    "            ax[i,E].plot(gProj[:,i,E][t_idxs[j]],linewidth=2.25,color=g_cMap[j,:,E])\n",
    "        \n",
    "            ax[i,E].plot([tgt_idx,tgt_idx],[-100,100],'gray',linewidth=.5)\n",
    "            ax[i,E].plot([move_idx,move_idx],[-100,100],'k',linewidth=.5)\n",
    "            ax[i,E].plot([ret_idx,ret_idx],[-100,100],'k',linewidth=.5)\n",
    "\n",
    "            ax[i,E].set_xlim([0,T+1])\n",
    "            ax[i,E].set_ylim([-120, 120])\n",
    "\n",
    "            if i<R_est-1:\n",
    "                ax[i,E].set_xticks([])\n",
    "            else:\n",
    "                ax[i,E].set_xlabel('Time (10ms bins)')\n",
    "            \n",
    "            ax[i,E].set_yticks([])\n",
    "            ax[i,E].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "        ax[0,E].set_title('Gamal Projections')\n",
    "\n",
    "# save figure\n",
    "\n",
    "# save directory\n",
    "figDir = '/Users/andrew/Documents/Projects/Churchland/Sparsity/figures/centerOutReaching/'\n",
    "\n",
    "# save\n",
    "plt.savefig(figDir + monkName + '_gamalProj.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
